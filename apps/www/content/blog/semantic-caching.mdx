---
date: 2024-06-06
title: "Using LLMs in production: harder than it seems"
image: "/images/blog-images/covers/llm-prod.png"
description: ""
author: dom
tags: ["product", "engineering"]
---

LLMs are powerful, useful, and getting better rapidly. But they're surprisingly hard to use in production:

- They're *non-deterministic*. In traditional computing, we expect the same input to produce the same output. But LLMs don't do this: they're more variable and human-like.
- They're often *slow*. Even using the fastest LLM available (at time of writing, that's Llama-70b hosted on Groq), we still need to make an HTTP call to a hosted API, waited for inference to complete, and return the result to the user. 
- They're *unreliable*. Complex LLM workflows (e.g. coding agents such as v0) involve multiple steps in a workflow, with retries if a given step fails. This multiplies latency. Aside from hallucinations, the high demand for hosted LLM APIs mean they often experience downtime.
- The landscape is *rapidly changing*. While OpenAI is leading the pack, the past year has seen a wide variety of challengers emerge: from competitive general-purpose closed source models like Claude Opus, open-source models like Llama-70b, and use-case-specific models (e.g. Cohere's Command R models, which are optimized for [retrieval](https://www.databricks.com/glossary/retrieval-augmented-generation-rag))

We're all familiar with the human-like power of applications like ChatGPT. But translating that power into new functionality that is useful to users in a consistent, reliable way is another matter. 

# Thesis:

AI is intrinsically unreliable. The way to stop that unreliability is through multi-step workflows. This increases latency and costs, which makes using gateways and caching useful.

# Unreliability

The past year has seen some amusing examples of large enterprises rushing to adopt LLMs in production, with unpredictable results: not necessariy in terms of downtime,
but in terms of 'hallucinations', where the model lies to the user. In other cases, the model lacks guardrails, so users are able to get it to act outside of the scope 
which it is intended for.

Some recent examples:

- Google introduced LLMs into the Google Search results, with the outcome that users received confusing search results including advice that 
  they should ["eat one small rock a day"](https://www.bbc.co.uk/news/articles/cd11gzejgz4o) or use glue to attach toppings to pizza.
- Delivery firm DPD introduced a chatbot which could be induced to [curse at the user](https://www.bbc.co.uk/news/technology-68025677), as well as delivering a haiku on the uselessness of its owner company
- Even ChatGPT fell prey to this in the last year; as load on its services** **increased, users noted that it became increasingly less useful for code generation. Rather than responding with the full code necessariy
  to build some functionality, it wouldn't instead give a rough guideline to the user and then helpfully suggest that they [draw the rest of the owl](https://www.reddit.com/r/funny/comments/eccj2/how_to_draw_an_owl/#lightbox).

# Complexity

Workflows like showing search results or having a chatbot retrieve relevant information for a customer aren't simple, but they're some of the more straightforward applications of 
large language models. The problem compounds as we start to move into more complex workflows, particularly those involving code generation. 

When retrieving information for a user, the demand for precision is relatively low. But if we're using an LLM to generate code, it's much higher: incorrect code will mean errors.

Often code generation involves multi-step workflows with LLMs. For example, take a look at the SFGPT repository here for an example:

1. Accept user input in the form of text
2. Translate this into SQL commands
3. Retrieve relevant SQL tables to query
4. Query the tables
5. If there was an error due to malformed, retry to attempt to fix it
6. Return the data to the user

Lots of steps, with lots more potential for error: 

https://x.com/garrytan/status/1799097037243498643

# Latency

Aside from reliability, this often means far higher latencies than users are accustomed to. While generating code with LLMs might be much faster than writing it yourself, it's
often necessary to wait seconds to receive a response. 

# Solutions

There are various tools out there that offer the ability to do intelligent caching of LLM responses: for instance, GPTCache. 

These allow you to cache an LLM response using the user query as a key. Rather than just constructing the cache key from the raw 
user query, they calculate an embedding of the user input and 